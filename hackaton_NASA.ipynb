{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento del dataset"
      ],
      "metadata": {
        "id": "JmJolBOpcAIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This celd make the processing of the dataset provide by NASA in https://wufs.wustl.edu/SpaceApps/data/space_apps_2024_seismic_detection.zip\n",
        "\n",
        "This is mandatory for the project pipeline because it returns the files processed with the labels of the seismic events.\n",
        "\n",
        "For runing, it is needed to stablish the follow variables: catalogo_filepath (path to the catalogs with the events)\n",
        "eventos_directory (path to the directory where are alocated the csv of the events)\n",
        "output_directory (path to the directory where the finals csv are saved)"
      ],
      "metadata": {
        "id": "OGLB0_bnvqTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Función para cargar el archivo del evento\n",
        "def cargar_evento(evento_filepath):\n",
        "    evento_df = pd.read_csv(evento_filepath)\n",
        "    # Convertir 'time_abs' a formato datetime\n",
        "    evento_df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'] = pd.to_datetime(evento_df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'],\n",
        "                                                                 format='%Y-%m-%dT%H:%M:%S.%f')\n",
        "    return evento_df\n",
        "\n",
        "# Función para encontrar archivos .csv en subcarpetas de manera recursiva\n",
        "def encontrar_archivo_csv(filename, eventos_directory):\n",
        "    for root, dirs, files in os.walk(eventos_directory):  # Recorre todas las carpetas y archivos\n",
        "        for file in files:\n",
        "            if file == f\"{filename}.csv\":\n",
        "                return os.path.join(root, file)  # Retorna la ruta completa del archivo si lo encuentra\n",
        "    return None\n",
        "\n",
        "# Función para combinar el evento con el catálogo y agregar etiqueta binaria\n",
        "def combinar_y_etiquetar_evento(catalogo_evento, eventos_directory):\n",
        "    # Obtener el nombre del archivo de evento desde el catálogo\n",
        "    filename = catalogo_evento['filename']\n",
        "\n",
        "    # Buscar el archivo correspondiente en las subcarpetas\n",
        "    evento_filepath = encontrar_archivo_csv(filename, eventos_directory)\n",
        "\n",
        "    # Verificar si el archivo existe\n",
        "    if evento_filepath is None:\n",
        "        print(f\"Archivo {filename}.csv no encontrado.\")\n",
        "        return None\n",
        "\n",
        "    # Cargar el archivo de evento\n",
        "    evento_df = cargar_evento(evento_filepath)\n",
        "\n",
        "    # Obtener el valor de 'time_rel' del catálogo que marca el inicio del evento\n",
        "    time_rel_inicio = catalogo_evento['time_rel(sec)']\n",
        "\n",
        "    # Crear una nueva columna 'label' con 0 en todas las filas\n",
        "    evento_df['label'] = 0\n",
        "\n",
        "    # Asignar 1 en la fila donde 'time_rel' coincida con el valor del catálogo\n",
        "    evento_df.loc[round(evento_df['time_rel(sec)']) == time_rel_inicio, 'label'] = 1\n",
        "\n",
        "    return evento_df\n",
        "\n",
        "# Función para crear el directorio si no existe\n",
        "def crear_directorio_si_no_existe(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Directorio '{directory}' creado.\")\n",
        "\n",
        "# Función principal para procesar los eventos desde el catálogo\n",
        "def procesar_catalogo(catalogo_filepath, eventos_directory, output_directory):\n",
        "    # Cargar el catálogo\n",
        "    catalogo_df = pd.read_csv(catalogo_filepath)\n",
        "\n",
        "    # Crear el directorio de salida si no existe\n",
        "    crear_directorio_si_no_existe(output_directory)\n",
        "\n",
        "    # Iterar sobre cada fila del catálogo\n",
        "    for _, catalogo_evento in catalogo_df.iterrows():\n",
        "        # Combinar y etiquetar\n",
        "        evento_etiquetado = combinar_y_etiquetar_evento(catalogo_evento, eventos_directory)\n",
        "\n",
        "        if evento_etiquetado is not None:\n",
        "            # Obtener el nombre del archivo de evento\n",
        "            filename = catalogo_evento['filename']\n",
        "            # Guardar el archivo con etiquetas\n",
        "            output_filepath = os.path.join(output_directory, f\"{filename}.csv\")\n",
        "            evento_etiquetado.to_csv(output_filepath, index=False)\n",
        "            print(f\"Evento {filename}.csv procesado y guardado en {output_filepath}\")\n",
        "\n",
        "# Ruta al archivo de catálogo y directorio de eventos\n",
        "catalogo_filepath = '' # space_apps_2024_seismic_detection/data/lunar/training/catalogs/apollo12_catalog_GradeA_final.csv\n",
        "eventos_directory = '' # space_apps_2024_seismic_detection/data/lunar/training/data\n",
        "output_directory = ''\n",
        "\n",
        "# Ejecutar el procesamiento\n",
        "procesar_catalogo(catalogo_filepath, eventos_directory, output_directory)"
      ],
      "metadata": {
        "id": "LlRcQaQxboEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corroborar que se hayan creado todas las etiquetas\n",
        "bin_files = [f for f in os.listdir(output_directory) if f.endswith('.csv')]\n",
        "for file in bin_files:\n",
        "    df = pd.read_csv(os.path.join(output_directory, file))\n",
        "    if sorted(df['label'].unique()) != [0, 1]:\n",
        "        print(f'En el archivo {file} falta etiqueta')"
      ],
      "metadata": {
        "id": "WVJc3ZaZcdv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "pdYdI4XpcDFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this training celd, the only variable to set is 'carpeta_eventos' with the same path to the directory where are the csv processed"
      ],
      "metadata": {
        "id": "n7-BMdhv4hSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsjwfCyTw88_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99b5986-8a24-4782-84d7-26edb9aac335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1/20\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.utils import resample\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def crear_directorio_si_no_existe(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Directorio '{directory}' creado.\")\n",
        "\n",
        "# Función para crear el modelo LSTM\n",
        "def crear_modelo_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(None, 1)))\n",
        "    model.add(LSTM(units=200, return_sequences=True))\n",
        "    model.add(Dropout(0.5))  # Regularización para evitar sobreajuste\n",
        "    model.add(LSTM(units=100, return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(LSTM(units=50, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(units=25, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))  # Salida para predicción\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Función para crear ventanas de secuencia a partir de un DataFrame\n",
        "def crear_ventanas_secuencia(df, window_size, step_size):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(0, len(df) - window_size + 1, step_size):\n",
        "        ventana = df.iloc[i:i + window_size]\n",
        "        X.append(ventana['velocity(m/s)'].values.reshape(-1, 1))\n",
        "        etiqueta_ventana = ventana['label'].max()  # Si hay al menos un '1', la ventana es 1\n",
        "        y.append(etiqueta_ventana)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Función para aplicar transformaciones ligeras a las secuencias (agregar ruido)\n",
        "def aplicar_transformaciones(X, ruido_factor=0.05):\n",
        "    X_transformado = X + ruido_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape)\n",
        "    return X_transformado\n",
        "\n",
        "# Función para sobremuestrear la clase minoritaria\n",
        "def sobremuestrear_clase_minoritaria(X, y):\n",
        "    # Separar clases\n",
        "    X_mayoritaria = X[y == 0]\n",
        "    X_minoritaria = X[y == 1]\n",
        "\n",
        "    # Sobremuestrear la clase minoritaria aplicando duplicación con ligeras transformaciones\n",
        "    X_minoritaria_duplicada = [aplicar_transformaciones(seq) for seq in X_minoritaria]\n",
        "\n",
        "    # Reescalar para igualar la cantidad de secuencias de la clase mayoritaria\n",
        "    X_minoritaria_duplicada = resample(X_minoritaria_duplicada, replace=True, n_samples=len(X_mayoritaria), random_state=42)\n",
        "\n",
        "    # Concatenar las secuencias originales con las sobremuestreadas\n",
        "    X_balanceado = np.concatenate([X_mayoritaria, X_minoritaria_duplicada], axis=0)\n",
        "    y_balanceado = np.array([0] * len(X_mayoritaria) + [1] * len(X_minoritaria_duplicada))\n",
        "\n",
        "    return X_balanceado, y_balanceado\n",
        "\n",
        "# Función para procesar archivos .csv en lotes\n",
        "def procesar_lote_de_eventos(lote_archivos, carpeta_eventos, window_size, step_size):\n",
        "    X_total = []\n",
        "    y_total = []\n",
        "\n",
        "    for archivo in lote_archivos:\n",
        "        if archivo.endswith('.csv'):\n",
        "            ruta_archivo = os.path.join(carpeta_eventos, archivo)\n",
        "            df_evento = pd.read_csv(ruta_archivo)\n",
        "            X, y = crear_ventanas_secuencia(df_evento, window_size, step_size)\n",
        "            X_total.append(X)\n",
        "            y_total.append(y)\n",
        "\n",
        "    X_total = np.concatenate(X_total, axis=0)\n",
        "    y_total = np.concatenate(y_total, axis=0)\n",
        "\n",
        "    return X_total, y_total\n",
        "\n",
        "# Función para procesar archivos .csv\n",
        "def procesar_eventos(lista_eventos, carpeta_eventos, window_size, step_size):\n",
        "    X_total = []\n",
        "    y_total = []\n",
        "\n",
        "    for archivo in lista_eventos:\n",
        "      df_evento = pd.read_csv(os.path.join(carpeta_eventos, archivo))\n",
        "      X, y = crear_ventanas_secuencia(df_evento, window_size, step_size)\n",
        "      X_total.append(X)\n",
        "      y_total.append(y)\n",
        "\n",
        "    X_total = np.concatenate(X_total, axis=0)\n",
        "    y_total = np.concatenate(y_total, axis=0)\n",
        "\n",
        "    return X_total, y_total\n",
        "\n",
        "# Función para calcular pesos de las clases\n",
        "def calcular_pesos_clases(y):\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "    return dict(enumerate(class_weights))\n",
        "\n",
        "# Entrenamiento por lotes\n",
        "def entrenar_por_lotes(carpeta_eventos, model, window_size, step_size, batch_size, num_epochs, save_path):\n",
        "    archivos_eventos = [f for f in os.listdir(carpeta_eventos) if f.endswith('.csv')]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Época {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for i in range(0, len(archivos_eventos), batch_size):\n",
        "            # Obtener un lote de archivos\n",
        "            lote_archivos = archivos_eventos[i:i + batch_size]\n",
        "\n",
        "            # Procesar el lote de eventos\n",
        "            X_total, y_total = procesar_lote_de_eventos(lote_archivos, carpeta_eventos, window_size, step_size)\n",
        "\n",
        "            # Aplicar sobremuestreo manual si hay desbalance en las clases\n",
        "            if len(np.unique(y_total)) == 2 and (np.sum(y_total == 1) / len(y_total)) < 0.5:\n",
        "                X_balanced, y_balanced = sobremuestrear_clase_minoritaria(X_total, y_total)\n",
        "            else:\n",
        "                X_balanced, y_balanced = X_total, y_total\n",
        "\n",
        "            # Calcular pesos de las clases para el conjunto actual\n",
        "            class_weights = calcular_pesos_clases(y_balanced)\n",
        "\n",
        "            # Entrenar el modelo en el lote actual\n",
        "            model.fit(X_balanced, y_balanced, epochs=10, batch_size=128, validation_split=0.3, verbose=1, class_weight=class_weights)\n",
        "\n",
        "        # Guardar el modelo después de cada época\n",
        "        model_name = os.path.join(save_path, f'lstm_lunar_model_epoch_{epoch + 1}.keras')\n",
        "        model.save(model_name)\n",
        "        print(f\"Fin de la época {epoch + 1}\")\n",
        "\n",
        "    # Guardar el modelo final\n",
        "    model.save(os.path.join(save_path, f'lstm_lunar_model.keras'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenamiento\n",
        "def entrenar(carpeta_eventos, model, window_size, step_size, num_epochs, save_path):\n",
        "    archivos_eventos = [f for f in os.listdir(carpeta_eventos) if f.endswith('.csv')]\n",
        "\n",
        "    # Procesar los eventos\n",
        "    X_total, y_total = procesar_eventos(archivos_eventos, carpeta_eventos, window_size, step_size)\n",
        "\n",
        "    # Aplicar sobremuestreo manual si hay desbalance en las clases\n",
        "    if len(np.unique(y_total)) == 2 and (np.sum(y_total == 1) / len(y_total)) < 0.5:\n",
        "        X_balanced, y_balanced = sobremuestrear_clase_minoritaria(X_total, y_total)\n",
        "    else:\n",
        "        X_balanced, y_balanced = X_total, y_total\n",
        "\n",
        "    # Calcular pesos de las clases para el conjunto actual\n",
        "    class_weights = calcular_pesos_clases(y_balanced)\n",
        "\n",
        "    # Definir el callback para guardar el mejor modelo en base a la métrica de validación\n",
        "    checkpoint = ModelCheckpoint(filepath='mejor_modelo.keras', monitor='val_loss',\n",
        "                                 verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
        "\n",
        "    # Entrenar el modelo en el lote actual\n",
        "    model.fit(X_balanced, y_balanced, epochs=num_epochs, batch_size=32,\n",
        "              validation_split=0.3, class_weight=class_weights,\n",
        "              callback = [checkpoint])\n",
        "\n",
        "    # Guardar el modelo\n",
        "    model_name = 'lstm_lunar_model.keras'\n",
        "    model.save(os.path.join(save_path, model_name))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Función para plotear métricas\n",
        "def plot_training_history(history):\n",
        "    # Graficar la pérdida (loss) durante el entrenamiento y validación\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
        "    plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
        "    plt.title('Pérdida durante el entrenamiento y validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Pérdida (Loss)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
        "    plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
        "    plt.title('Precisión durante el entrenamiento y validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Precisión (Accuracy)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Mostrar gráficos\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Parámetros\n",
        "window_size = 100\n",
        "step_size = 10\n",
        "batch_size = 5  # Procesar 5 archivos por lote\n",
        "num_epochs = 10\n",
        "carpeta_eventos = '' # Same path to the directory where are the csv processed\n",
        "\n",
        "# Directorio donde se encuetran los modelos\n",
        "actual_time = datetime.now()\n",
        "model_path = ''\n",
        "crear_directorio_si_no_existe(model_path)\n",
        "\n",
        "# Crear el modelo o cargar uno previamente entrenado\n",
        "model_name = 'lstm_lunar_model.keras'\n",
        "if os.path.exists(os.path.join(model_path, model_name)):\n",
        "    model = load_model(os.path.join(model_path, model_name))\n",
        "    print(f\"Modelo cargado desde {os.path.join(model_path, model_name)}\")\n",
        "else:\n",
        "    model = crear_modelo_lstm()\n",
        "\n",
        "# Entrenamiento\n",
        "history = entrenar_por_lotes(carpeta_eventos, model, window_size, step_size, batch_size, num_epochs, model_path)\n",
        "#history = entrenar(carpeta_eventos, model, window_size, step_size, num_epochs, model_path)\n",
        "\n",
        "# Ploteo de graficas\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación del modelo"
      ],
      "metadata": {
        "id": "TLjoz2hYcFxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the evaluation of the model is necessary to set the following variables:\n",
        "\n",
        "graphs_path (path to save the graphs plotted)\n",
        "carpeta_test (path where are the directories of test dataset)\n",
        "model_path (same path where is alocate the best model)\n",
        "catalogo_path (path to the output catalog with the detections)"
      ],
      "metadata": {
        "id": "vKhlSK1G5IvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9cyG1s_z70_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Función para crear ventanas de secuencia\n",
        "def crear_ventanas_secuencia_prediccion(df, window_size, step_size, columna):\n",
        "    X = []\n",
        "    indices = []\n",
        "\n",
        "    # Crear ventanas deslizantes a partir de la columna especificada\n",
        "    for i in range(0, len(df) - window_size + 1, step_size):\n",
        "        ventana = df[columna].iloc[i:i + window_size]\n",
        "        X.append(ventana.values.reshape(-1, 1))  # Reshape para que sea compatible con la LSTM\n",
        "        indices.append(i)  # Guardar el índice inicial de cada ventana\n",
        "\n",
        "    return np.array(X), np.array(indices)\n",
        "\n",
        "# Función para graficar predicciones y marcar el inicio predicho por el modelo\n",
        "def graficar_predicciones(df, predicciones, indice_ventana_max, window_size, graphs_path):\n",
        "    file_name = os.path.basename(archivo)\n",
        "    save_path = os.path.join(graphs_path, os.path.splitext(file_name)[0])\n",
        "\n",
        "    # Graficar la señal de velocidad\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df.index, df['velocity(m/s)'], label='Señal de Velocidad')\n",
        "\n",
        "    # Resaltar el inicio predicho (ventana donde se alcanzó el máximo)\n",
        "    inicio_predicho = indice_ventana_max\n",
        "    plt.axvline(x=inicio_predicho, color='red', linestyle='--', label='Inicio Predicho')\n",
        "\n",
        "    plt.title(f'Predicciones para archivo {file_name}')\n",
        "    plt.xlabel('Índice de Tiempo')\n",
        "    plt.ylabel('Velocidad (m/s)')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{save_path}.png', dpi=300, bbox_inches='tight')\n",
        "    #plt.show()\n",
        "\n",
        "# Parámetros\n",
        "window_size = 100\n",
        "step_size = 10\n",
        "graphs_path = ''\n",
        "carpeta_test = '' # space_apps_2024_seismic_detection/data/lunar/test/data\n",
        "model_path = ''\n",
        "catalogo_path = ''\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Lista para almacenar las predicciones\n",
        "predicciones_catalogo = []\n",
        "\n",
        "# Procesar archivos de test\n",
        "subcarpetas_test = [f for f in os.listdir(carpeta_test)]\n",
        "for subcarpeta in subcarpetas_test:\n",
        "    subcarpeta_path = os.path.join(carpeta_test, subcarpeta)\n",
        "    archivos_test = [f for f in os.listdir(subcarpeta_path) if f.endswith('.csv')]\n",
        "    for archivo in archivos_test:\n",
        "        ruta_archivo = os.path.join(subcarpeta_path, archivo)\n",
        "        df_test = pd.read_csv(ruta_archivo)\n",
        "\n",
        "        # Crear ventanas de secuencia para testeo\n",
        "        X_test, indices = crear_ventanas_secuencia_prediccion(df_test, window_size, step_size, columna='velocity(m/s)')\n",
        "\n",
        "        # Hacer predicciones\n",
        "        predicciones = model.predict(X_test)\n",
        "\n",
        "        # Encontrar la ventana con la predicción más alta\n",
        "        indice_ventana_max = indices[np.argmax(predicciones)]\n",
        "\n",
        "        # Calcular el tiempo absoluto y tiempo relativo\n",
        "        tiempo_abs = df_test['time_abs(%Y-%m-%dT%H:%M:%S.%f)'].iloc[indice_ventana_max]\n",
        "        tiempo_abs = pd.to_datetime(tiempo_abs).strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
        "\n",
        "        tiempo_rel = (indice_ventana_max * step_size)  # Tiempo relativo en segundos\n",
        "\n",
        "        # Guardar la información en la lista del catálogo\n",
        "        predicciones_catalogo.append([archivo, tiempo_rel])\n",
        "\n",
        "        # Graficar los resultados\n",
        "        graficar_predicciones(df_test, predicciones, indice_ventana_max, window_size, graphs_path)\n",
        "\n",
        "# Crear un DataFrame y guardar en CSV\n",
        "df_catalogo = pd.DataFrame(predicciones_catalogo, columns=['filename', 'time_rel(sec)'])\n",
        "df_catalogo.to_csv(catalogo_path, index=False)\n",
        "\n",
        "print(f\"Catálogo de predicciones guardado en {catalogo_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}